### Proyecto Apache Haddop

#### Introduccion
El proyecto Apache Hadoop es un marco de software de código abierto diseñado para almacenar y procesar grandes volúmenes de datos de manera distribuida y paralela en un clúster de computadoras estándar (es decir, computadoras comunes conectadas por red). Busca facilitar el procesamiento de Big Data en entornos distribuidos, es decir , trabajar con terabytes o petabytes de datos de forma eficiente, escalable y tolerante a fallos, esto sin necesidad de supercomputadoras, basta muchos equipos modestos conectados Apache Hadoop está basado en dos pilares principales:
- HDFS (Hadoop Distributed File System)
Es el sistema de archivos distribuido de Hadoop. Se encarga de:
    - Dividir archivos grandes en bloques (por ejemplo, de 128 MB).
    - Guardar copias de esos bloques en distintos nodos del clúster (tolerancia a fallos).
    - Permitir acceso rápido y distribuido a los datos.
- MapReduce
Es el modelo de programación que permite procesar los datos de forma distribuida. Se compone de:
    - Una función Map: divide el problema en tareas pequeñas que se ejecutan en paralelo.
    - Una función Reduce: agrupa y combina los resultados del map.
Hadoop se encarga de distribuir, ejecutar y reintentar tareas en caso de fallos.

#### Componentes del Ecosistema Hadoop
Además de HDFS y MapReduce, el proyecto Hadoop incluye otros proyectos complementarios:
- YARN: Coordina y administra los recursos del clúster.
- HDFS: Sistema de archivos distribuido.
- MapReduce: Procesamiento distribuido de grandes volúmenes de datos.
- Hive: Lenguaje de consultas tipo SQL sobre Hadoop.
- Pig: Lenguaje de scripts de alto nivel para análisis de datos.
- HBase: Base de datos NoSQL distribuida sobre HDFS.
- Spark: Procesamiento en memoria más rápido que MapReduce (alternativa).

#### Resumen
El proyecto Apache Hadoop es una popular implementación de código abierto
del paradigma Map-Reduce para computación distribuida. Un ordenador distribuido puede
verse como un gran conjunto de ordenadores multinúcleo conectados por una red, de tal
forma que cada ordenador tiene varios núcleos de procesador, por ejemplo, P0, P1, P2, P3.
Cada ordenador individual dispone también de algún tipo de almacenamiento persistente
(por ejemplo, disco duro, memoria flash), lo que permite almacenar y operar con grandes
volúmenes de datos al agregar el almacenamiento disponible en todos los ordenadores de
un centro de datos. La principal motivación del proyecto Hadoop es facilitar la escritura de
programas paralelos a gran escala que operen con estos "grandes datos".

El marco Hadoop permite al programador especificar funciones de mapeo y reducción en
Java, y se encarga de todos los detalles de generar un gran número de tareas de mapeo y
reducción para realizar el cálculo, así como de programarlas en un ordenador distribuido.
Una propiedad clave del marco Hadoop es que admite la tolerancia automática a fallos.
Dado que MapReduce es esencialmente un modelo de programación funcional, si un nodo
del sistema distribuido falla, el programador de Hadoop puede reprogramar las tareas que
se estaban ejecutando en ese nodo con la misma entrada en otro lugar y continuar el
cómputo. Esto no es posible con el paralelismo no funcional en general, porque cuando una
tarea no funcional modifica algún estado, volver a ejecutarla puede dar como resultado una
respuesta diferente. La capacidad del marco Hadoop para procesar volúmenes masivos de
datos también lo ha convertido en un objetivo popular para los lenguajes de consulta de
alto nivel que implementan una semántica similar a SQL sobre Hadoop.

En la conferencia se analizó el ejemplo del recuento de palabras, que, a pesar de su
simplicidad, se utiliza muy a menudo en la práctica para la minería de documentos y la
minería de textos. En este ejemplo, se ilustró cómo un programa map-reduce de Hadoop
puede obtener recuentos de palabras para el texto distribuido "Ser o no ser". Hay muchas
otras aplicaciones que se han construido sobre Hadoop y otros marcos MapReduce. La
principal ventaja de Hadoop es que simplifica enormemente el trabajo de escribir
programas para procesar grandes volúmenes de datos disponibles en un centro de datos.