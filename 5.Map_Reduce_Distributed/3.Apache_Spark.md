### Apache Spark

Apache Spark es un modelo de programación similar, pero más general, que
Hadoop MapReduce. Al igual que Hadoop, Spark también funciona en sistemas
distribuidos, pero una diferencia clave en Spark es que hace un mejor uso de la
computación en memoria dentro de los nodos distribuidos en comparación con Hadoop
MapReduce. Esta diferencia puede tener un impacto significativo en el rendimiento de los
algoritmos MapReduce iterativos, ya que el uso de la memoria evita la necesidad de
escribir los resultados intermedios en el almacenamiento externo después de cada paso
de map/reduce. Sin embargo, esto también implica que el tamaño de los datos que
pueden procesarse de esta manera está limitado por el tamaño total de la memoria en
todos los nodos, que suele ser mucho menor que el tamaño del almacenamiento externo.
(Spark puede volcar el exceso de datos al almacenamiento externo si es necesario, pero al
hacerlo se reduce la ventaja de rendimiento sobre Hadoop)
Otra diferencia importante entre Spark y Hadoop MapReduce, es que el tipo de datos
principal en Spark es el conjunto de datos distribuido resistente (RDD), que puede verse
como una generalización de los conjuntos de pares clave-valor. Los RDD permiten a Spark
soportar operaciones más generales que map y reduce. Spark admite operaciones
intermedias denominadas transformaciones (por ejemplo, map,filter,join,...) y
operaciones terminales denominadas acciones (por ejemplo,
reduce,count,collect,...). Al igual que en los flujos de Java, las transformaciones
intermedias se realizan de forma perezosa, es decir, su evaluación se pospone hasta el
momento en que es necesario realizar una acción terminal.
El recuento de palabras puede implementarse en Spark utilizando las API de Java. 
(Las API de Java utilizan la misma implementación subyacente
que las API de Scala, ya que ambas API pueden invocarse en la misma instancia de la
máquina virtual de Java) Utilizamos el método Spark flatMap() para combinar todas las
palabras de todas las líneas de un archivo de entrada en un único RDD, seguido de una
llamada al método mapToPair() Transform para emitir pares de la forma,(palabra, 1),
que luego pueden ser procesados por una operación reduceByKey() para obtener el
recuento final de palabras.